# Self-attention building blocks for computer vision applications

Implementation of self attention mechanisms for general purpose. Focused on computer vision modules. Ongoing repository. pip package coming soon...



Modules implemented so far:
- Scaled dot product self attention
- Multi-head-self-attention
- Axial attention and axial attention residual block
- Local attention
- Botleneck attention 
