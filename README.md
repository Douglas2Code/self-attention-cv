# Self-attention building blocks for computer vision applications in PyTorch

Implementation of self attention mechanisms for general purpose in PyTorch. 

Focused on computer vision self-attention modules. 

Ongoing repository. 





pip package coming soon...

## Attention modules

Modules implemented so far:
- Scaled dot product self attention
- Multi-head-self-attention
- Axial attention and axial attention residual block
- Local attention
- Botleneck self-attention 
