# Self-attention building blocks for computer vision applications in PyTorch

Implementation of self attention mechanisms for general purpose in PyTorch with einsum and einops.rearrange


Focused on computer vision self-attention modules. 

Ongoing repository. pip package coming soon...

## Background on attention and transformers
- [How Attention works in Deep Learning](https://theaisummer.com/attention/)
- [How Transformers work in deep learning and NLP](https://theaisummer.com/transformer/)
- How to implement multi-head self-attention blocks in PyTorch using the einsum notation





### Attention modules implemented so far:
- Scaled dot product self attention
- Multi-head-self-attention
- Axial attention and axial attention residual block

### TODO
- Local attention
- Botleneck self-attention 


### Code Examples